from keras.layers import Input, Dense, Activation, BatchNormalization, LeakyReLU, Dropout, Maximum, Concatenate
from keras.models import Model, load_model
# from keras.optimizers import Adam
from tensorflow.keras.optimizers.legacy import Adam, SGD
# from tensorflow.keras import backend as K
from VOTEClassifier import VOTEClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import linear_model, svm, tree
from sklearn.model_selection import train_test_split
from collections import defaultdict
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import argparse
import _pickle as cPickle
import gzip
import os, sys
import pickle
import re
import math
import lief
import pandas as pd
import zipfile
import tempfile
import json
import random
import time
import copy

parser = argparse.ArgumentParser()
parser.add_argument("-t", "--train", nargs = 5, help="--train <malware_dir> <goodware_dir> <num_features> <train_size> <cur_fold_num>")
parser.add_argument("-p", "--test", nargs = 5, help="--test <file> <num_features> <train_size> <cur_fold_num> <pred_save_path>")
parser.add_argument("-a", "--attack", nargs = 5, help="--attack <malware_file> <num_features> <train_size> <cur_fold_num> <json_save_path>")
args = parser.parse_args()

if args.train is not None:
    train_size = int(args.train[3])
    cur_fold = int(args.train[4])
    train_files = {}
elif args.test is not None:
    train_size = int(args.test[2])
    cur_fold = int(args.test[3])
elif args.attack is not None:
    train_size = int(args.attack[2])
    cur_fold = int(args.attack[3])

class PEAttributeExtractor():

    libraries = ""
    functions = ""
    exports = ""

    # initialize extractor
    def __init__(self, bytez):
        # save bytes
        self.bytez = bytez
        # parse using lief
        self.lief_binary = lief.PE.parse(list(bytez))
        # attributes
        self.attributes = {}
        self.grouped_data = {}
    
    # extract attributes
    def extract(self):
        
        # get imported libraries and functions
        if self.lief_binary.has_imports:
            self.libraries = " ".join([l for l in self.lief_binary.libraries])
            # self.functions = " ".join([f.name for f in self.lief_binary.imported_functions])
        self.functions = []
        self.grouped_data.update({"libraries": {}})
        for imported_library in self.lief_binary.imports:
              self.grouped_data["libraries"].update({imported_library.name: []})
              for func in imported_library.entries:
                if not func.is_ordinal:
                  self.grouped_data["libraries"][imported_library.name].append(func.name)
                  self.functions.append(func.name)
        self.functions = " ".join(self.functions)
        
        self.attributes.update({"functions": self.functions, "libraries": self.libraries})

        # get exports
        # if self.lief_binary.has_exports:
        #     self.exports = [f.name for f in self.lief_binary.exported_functions]
        self.grouped_data.update({"exports_list": self.exports})
        self.exports = " ".join(self.exports)
        self.attributes.update({"exports_list": self.exports})

        return (self.attributes, self.grouped_data)

class FeatureExtractor():

    def feature_extractor_dir(self, sampled_files, label, return_after_grouping = 0):
        # if not os.path.exists(input_dir):
        #   print("!!!INPUT DIR (" + input_dir + ") NOT FOUND!!!")
        #   return
        data = []
        grouped_data = {"libraries": {}, "exports_list":[]}
        for _file in sampled_files:
          filepath = _file
          try:
            pe_att_ext = PEAttributeExtractor(open(filepath,'rb').read())
            returned_data = pe_att_ext.extract()
            atts = returned_data[0]
            atts['label'] = label
            data.append(atts)
            grouped_ext = returned_data[1]
            if not grouped_data:
              grouped_data = grouped_ext
              continue
            for _library in grouped_ext["libraries"]:
              if not _library in grouped_data["libraries"].keys():
                grouped_data["libraries"].update({_library: grouped_ext["libraries"][_library]})
              else:
                for func in grouped_ext["libraries"][_library]:
                  if not func in grouped_ext["libraries"][_library]:
                    grouped_ext["libraries"][_library].append(func)
            for _export in grouped_ext["exports_list"]:
              if not _export in grouped_data["exports_list"]:
                grouped_data["exports_list"].append(_export)
          except:
            pass
        if return_after_grouping:
          return grouped_data
        return data
    
    def feature_extractor_file(self, input_file):
        if not os.path.exists(input_file):
          print("!!!INPUT FILE (" + input_file + ") NOT FOUND!!!")
          return
        file_data = []
        try:
          pe_att_ext = PEAttributeExtractor(open(input_file,'rb').read())
          atts = pe_att_ext.extract()[0]
          # atts['label'] = label
          file_data.append(atts)
        except:
          pass
        return file_data


class FeatureEstimator():
    def list_files(self, directory):
        file_paths = []
        for root, _, files in os.walk(directory):
            for file in files:
                file_paths.append(os.path.join(root, file))
        return file_paths

    def num_features_estimator(self, given_num_f):
        malware_dir, goodware_dir = args.train[0], args.train[1]
        malware_files = self.list_files(malware_dir)
        goodware_files = self.list_files(goodware_dir)
        feature_space = set()
        feat_ext = FeatureExtractor()

        # for _dir in malware_dirs:
        # input_dir = os.path.join(malware_dir, _dir)
        sampled_malware_files = random.sample(malware_files, train_size)
        data = feat_ext.feature_extractor_dir(sampled_malware_files, 1)
        for _dict in data:
          _dict.pop('label', None)
          for key in _dict:
            value = _dict[key]
            features = value.split()
            for feature in features:
              feature_space.add(feature)

        # for _dir in goodware_dirs:
        #   input_dir = os.path.join(goodware_dir, _dir)
        sampled_goodware_files = random.sample(goodware_files, train_size)
        data = feat_ext.feature_extractor_dir(sampled_goodware_files, 0)
        for _dict in data:
          _dict.pop('label', None)
          for key in _dict:
            value = _dict[key]
            features = value.split()
            for feature in features:
              feature_space.add(feature)
        
        global train_files
        train_files['malware'] = sampled_malware_files
        train_files['goodware'] = sampled_goodware_files

        feature_space = list(feature_space)
        feature_space.sort()
        len_feat_space = len(feature_space)
        obj_len_feat_space = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/len_feat_space.obj'
        if not os.path.exists(obj_len_feat_space):
          os.makedirs('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '', exist_ok = True)
          with open(obj_len_feat_space, 'wb') as lf_file:
            pickle.dump(len_feat_space, lf_file)
        
        obj_train_files = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/train_files.obj'
        if not os.path.exists(obj_train_files):
          os.makedirs('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '', exist_ok = True)
          with open(obj_train_files, 'wb') as lf_file:
            pickle.dump(train_files, lf_file)
        num_f = min(given_num_f, len(feature_space)) if given_num_f > 0 else len(feature_space)
        return int(num_f)


class MalGAN():
    def __init__(self, apifeature_dims, blackbox='RF', same_train_data=1, filename='data.npz'):
        self.apifeature_dims = apifeature_dims
        self.z_dims = 20
        self.hide_layers = 256
        self.generator_layers = [self.apifeature_dims+self.z_dims, self.hide_layers, self.apifeature_dims]
        self.substitute_detector_layers = [self.apifeature_dims, self.hide_layers, 1]
        self.blackbox = blackbox       # RF LR DT SVM MLP VOTE
        self.same_train_data = same_train_data   # MalGAN and the black-boxdetector are trained on same or different training sets
        self.lr = 0.001
        try:
          optimizer = Adam(learning_rate=self.lr)
        except:
          optimizer = SGD(learning_rate=self.lr)
        self.filename = filename
        if args.train:
          self.malware_dir = args.train[0]
          self.goodware_dir = args.train[1]

        # Build and Train blackbox_detector
        self.blackbox_detector = self.build_blackbox_detector()

        # Build and compile the substitute_detector
        self.substitute_detector = self.build_substitute_detector()
        self.substitute_detector.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

        # Build the generator
        self.generator = self.build_generator()

        # The generator takes malware and noise as input and generates adversarial malware examples
        example = Input(shape=(self.apifeature_dims,))
        noise = Input(shape=(self.z_dims,))
        input = [example, noise]
        malware_examples = self.generator(input)

        # For the combined model we will only train the generator
        self.substitute_detector.trainable = False

        # The discriminator takes generated images as input and determines validity
        validity = self.substitute_detector(malware_examples)

        # The combined model  (stacked generator and substitute_detector)
        # Trains the generator to fool the discriminator
        self.combined = Model(input, validity)
        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)

    def build_blackbox_detector(self):

        if self.blackbox is 'RF':
            blackbox_detector = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=1)
        elif self.blackbox is 'SVM':
            blackbox_detector = svm.SVC()
        elif self.blackbox is 'LR':
            blackbox_detector = linear_model.LogisticRegression()
        elif self.blackbox is 'DT':
            blackbox_detector = tree.DecisionTreeRegressor()
        elif self.blackbox is 'MLP':
            blackbox_detector = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
                                              solver='sgd', verbose=0, tol=1e-4, random_state=1,
                                              learning_rate_init=.1)
        elif self.blackbox is 'VOTE':
            blackbox_detector = VOTEClassifier()

        return blackbox_detector

    def build_generator(self):

        example = Input(shape=(self.apifeature_dims,))
        noise = Input(shape=(self.z_dims,))
        x = Concatenate(axis=1)([example, noise])
        for dim in self.generator_layers[1:]:
            x = Dense(dim)(x)
        x = Activation(activation='sigmoid')(x)
        # x = Maximum()([example, x])
        generator = Model([example, noise], x, name='generator')
        generator.summary()
        return generator

    def build_substitute_detector(self):
        
        input = Input(shape=(self.substitute_detector_layers[0],))
        x = input
        for dim in self.substitute_detector_layers[1:]:
            x = Dense(dim)(x)
        x = Activation(activation='sigmoid')(x)
        substitute_detector = Model(input, x, name='substitute_detector')
        substitute_detector.summary()
        return substitute_detector

    def list_files(self, directory):
        file_paths = []
        for root, _, files in os.walk(directory):
            for file in files:
                file_paths.append(os.path.join(root, file))
        return file_paths

    def load_data(self):
        with open('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/train_files.obj', 'rb') as lf_file:
            train_files = pickle.load(lf_file)

        # train_files['goodware'] = train_files['goodware'][:1000]
        # train_files['malware'] = train_files['malware'][:1000]

        # malware_dirs = os.listdir(self.malware_dir)
        # goodware_dirs = os.listdir(self.goodware_dir)
        # malware_files = self.list_files(self.malware_dir)
        # goodware_files = self.list_files(self.goodware_dir)

        feature_space = defaultdict(lambda: 0)
        feat_ext = FeatureExtractor()
        dict_feat_space = {"libraries": {}, "exports_list":[]}
        func_lib_map = {}
        # for _dir in malware_dirs:
        # input_dir = os.path.join(self.malware_dir, _dir)
        # data = feat_ext.feature_extractor_dir(input_dir, 1)
        # grouped_ext = feat_ext.feature_extractor_dir(input_dir, 1, 1)
        sampled_malware_files = train_files['malware']
        data_malware = feat_ext.feature_extractor_dir(sampled_malware_files, 1)
        grouped_ext = feat_ext.feature_extractor_dir(sampled_malware_files, 1, 1)
        if not dict_feat_space:
          dict_feat_space = grouped_ext
        else:
          for _library in grouped_ext["libraries"]:
            if not _library in dict_feat_space["libraries"].keys():
              dict_feat_space["libraries"].update({_library: grouped_ext["libraries"][_library]})
            else:
              for func in grouped_ext["libraries"][_library]:
                if not func in grouped_ext["libraries"][_library]:
                  grouped_ext["libraries"][_library].append(func)
                  func_lib_map[func] = _library
          for _export in grouped_ext["exports_list"]:
            if not _export in dict_feat_space["exports_list"]:
              dict_feat_space["exports_list"].append(_export)
        for _dict in data_malware:
          _dict.pop('label', None)
          for key in _dict:
            value = _dict[key]
            features = value.split()
            for feature in features:
              feature_space[feature] += 1

        # for _dir in goodware_dirs:
        # input_dir = os.path.join(self.goodware_dir, _dir)
        # data = feat_ext.feature_extractor_dir(input_dir, 0)
        # grouped_ext = feat_ext.feature_extractor_dir(input_dir, 0, 1)
        sampled_goodware_files = train_files['goodware']
        data_goodware = feat_ext.feature_extractor_dir(sampled_goodware_files, 0)
        grouped_ext = feat_ext.feature_extractor_dir(sampled_goodware_files, 0, 1)
        if not dict_feat_space:
          dict_feat_space = grouped_ext
        else:
          for _library in grouped_ext["libraries"]:
            if not _library in dict_feat_space["libraries"].keys():
              dict_feat_space["libraries"].update({_library: grouped_ext["libraries"][_library]})
            else:
              for func in grouped_ext["libraries"][_library]:
                if not func in grouped_ext["libraries"][_library]:
                  grouped_ext["libraries"][_library].append(func)
                  func_lib_map[func] = _library
          for _export in grouped_ext["exports_list"]:
            if not _export in dict_feat_space["exports_list"]:
              dict_feat_space["exports_list"].append(_export)
        for _dict in data_goodware:
          _dict.pop('label', None)
          for key in _dict:
            value = _dict[key]
            features = value.split()
            for feature in features:
              feature_space[feature] += 1
        
        feature_space = sorted(feature_space.items(), key=lambda x:x[1])
        feature_space.reverse()
        # feature_space = feature_space[:self.apifeature_dims]
        f_space = set()
        for feature in feature_space[:self.apifeature_dims]:
          if len(f_space) >= self.apifeature_dims:
            break
          if feature[0] in func_lib_map.keys():
            f_space.add(func_lib_map[feature[0]])
          if len(f_space) >= self.apifeature_dims:
            break
          f_space.add(feature[0])

        if len(f_space) != self.apifeature_dims:
          print("!!!NOT POSSIBLE!!!")
          exit(1)

        # feature_space = [i[0] for i in feature_space]
        feature_space = list(f_space)

        xmal, ymal, xben, yben = [], [], [], []
        
        # for _dir in malware_dirs:
        # input_dir = os.path.join(self.malware_dir, _dir)
        # data = feat_ext.feature_extractor_dir(input_dir, 1)
        # sampled_malware_files = train_files['malware']
        # data = feat_ext.feature_extractor_dir(sampled_malware_files, 1)
        for _dict in data_malware:
          ymal.append(1)
          _dict.pop('label', None)
          feature_vector = dict.fromkeys(feature_space, 0)
          for key in _dict:
            value = _dict[key]
            features = value.split()
            for feature in features:
              if feature in feature_space:
                feature_vector[feature] = 1
          values = list(feature_vector.values())    
          xmal.append(values)
        
        # for _dir in goodware_dirs:
        # input_dir = os.path.join(self.goodware_dir, _dir)
        # data = feat_ext.feature_extractor_dir(input_dir, 0)
        # sampled_goodware_files = train_files['goodware']
        # data = feat_ext.feature_extractor_dir(sampled_goodware_files, 0)
        for _dict in data_goodware:
          yben.append(0)
          _dict.pop('label', None)
          feature_vector = dict.fromkeys(feature_space, 0)
          for key in _dict:
            value = _dict[key]
            features = value.split()
            for feature in features:
              if feature in feature_space:
                feature_vector[feature] = 1
          values = list(feature_vector.values())    
          xben.append(values)
        
        obj_dict_fp = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/dict_feat.obj'
        os.makedirs('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '', exist_ok = True)
        with open(obj_dict_fp, 'wb') as df_file:
          pickle.dump(dict_feat_space, df_file)

        obj_fp = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(self.apifeature_dims) + '/feature_space.obj'
        os.makedirs('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(self.apifeature_dims), exist_ok = True)
        with open(obj_fp, 'wb') as feature_space_file:
          pickle.dump(feature_space, feature_space_file)

        xmal = np.array(xmal)
        ymal = np.array(ymal)
        xben = np.array(xben)
        yben = np.array(yben)
        np.savez('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims)+'/'+self.filename, xmal=xmal, ymal=ymal, xben=xben, yben=yben)
        
        print(len(sampled_malware_files))
        print(len(sampled_goodware_files))

        return (xmal, ymal), (xben, yben)

    def train(self, epochs, batch_size=32, is_first=1):

        # Load and Split the dataset
        (xmal, ymal), (xben, yben) = self.load_data()
        # print(xmal)
        # time.sleep(30)
        # print(ymal)
        # time.sleep(30)
        # print(xben)
        # time.sleep(30)
        # print(yben)
        # time.sleep(30)
        # print(np.shape(xmal), np.shape(ymal), np.shape(xben), np.shape(yben))
        # time.sleep(30)

        # obj_xymalben = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(self.apifeature_dims) + '/xmal_ymal_xben_yben.obj'
        # os.makedirs('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(self.apifeature_dims), exist_ok = True)
        # with open(obj_xymalben, 'wb') as xymalben_file:
        #   pickle.dump([xmal, ymal, xben, yben], xymalben_file)

        xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
        xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
        if self.same_train_data:
            bl_xtrain_mal, bl_ytrain_mal, bl_xtrain_ben, bl_ytrain_ben = xtrain_mal, ytrain_mal, xtrain_ben, ytrain_ben
        else:
            xtrain_mal, bl_xtrain_mal, ytrain_mal, bl_ytrain_mal = train_test_split(xtrain_mal, ytrain_mal, test_size=0.50)
            xtrain_ben, bl_xtrain_ben, ytrain_ben, bl_ytrain_ben = train_test_split(xtrain_ben, ytrain_ben, test_size=0.50)

        # if is_first is Ture, Train the blackbox_detctor
        if is_first:
            self.blackbox_detector.fit(np.concatenate([xmal, xben]),
                                       np.concatenate([ymal, yben]))
            obj_blackbox_detector = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims)+'/blackbox_detector.pkl'
            os.makedirs('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims), exist_ok = True)
            with open(obj_blackbox_detector, 'wb') as lf_file:
              pickle.dump(self.blackbox_detector, lf_file)

        ytrain_ben_blackbox = self.blackbox_detector.predict(bl_xtrain_ben)
        Original_Train_TPR = self.blackbox_detector.score(bl_xtrain_mal, bl_ytrain_mal)
        Original_Test_TPR = self.blackbox_detector.score(xtest_mal, ytest_mal)
        Train_TPR, Test_TPR = [Original_Train_TPR], [Original_Test_TPR]
        best_TPR = 1.0
        for epoch in range(epochs):

            for step in range(xtrain_mal.shape[0] // batch_size):
                # ---------------------
                #  Train substitute_detector
                # ---------------------

                # Select a random batch of malware examples
                idx = np.random.randint(0, xtrain_mal.shape[0], batch_size)
                xmal_batch = xtrain_mal[idx]
                noise = np.random.uniform(0, 1, (batch_size, self.z_dims))
                idx = np.random.randint(0, xmal_batch.shape[0], batch_size)
                xben_batch = xtrain_ben[idx]
                yben_batch = ytrain_ben_blackbox[idx]

                # Generate a batch of new malware examples
                gen_examples = self.generator.predict([xmal_batch, noise])
                ymal_batch = self.blackbox_detector.predict(np.ones(gen_examples.shape)*(gen_examples > 0.5))

                # Train the substitute_detector
                d_loss_real = self.substitute_detector.train_on_batch(gen_examples, ymal_batch)
                d_loss_fake = self.substitute_detector.train_on_batch(xben_batch, yben_batch)
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

                # ---------------------
                #  Train Generator
                # ---------------------

                idx = np.random.randint(0, xtrain_mal.shape[0], batch_size)
                xmal_batch = xtrain_mal[idx]
                noise = np.random.uniform(0, 1, (batch_size, self.z_dims))

                # Train the generator
                g_loss = self.combined.train_on_batch([xmal_batch, noise], np.zeros((batch_size, 1)))

            # Compute Train TPR
            noise = np.random.uniform(0, 1, (xtrain_mal.shape[0], self.z_dims))
            gen_examples = self.generator.predict([xtrain_mal, noise])
            TPR = self.blackbox_detector.score(np.ones(gen_examples.shape) * (gen_examples > 0.5), ytrain_mal)
            Train_TPR.append(TPR)

            # Compute Test TPR
            noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.z_dims))
            gen_examples = self.generator.predict([xtest_mal, noise])
            TPR = self.blackbox_detector.score(np.ones(gen_examples.shape) * (gen_examples > 0.5), ytest_mal)
            Test_TPR.append(TPR)

            # Save best model
            if TPR < best_TPR:
                self.combined.save('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims)+'/combined_model_malgan.keras')
                self.generator.save('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims)+'/generator_malgan.keras')
                self.substitute_detector.save('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims)+'/substitute_detector_malgan.keras')
                # self.blackbox_detector.save('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims)+'/blackbox_detector.keras')
                best_TPR = TPR

            # Plot the progress
            if is_first:
                print("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))

        flag = ['DiffTrainData', 'SameTrainData']
        print('\n\n---{0} {1}'.format(self.blackbox, flag[self.same_train_data]))
        print('\nOriginal_Train_TPR: {0}, Adver_Train_TPR: {1}'.format(Original_Train_TPR, Train_TPR[-1]))
        print('\nOriginal_Test_TPR: {0}, Adver_Test_TPR: {1}'.format(Original_Test_TPR, Test_TPR[-1]))

        # Plot TPR
        plt.figure()
        plt.plot(range(len(Train_TPR)), Train_TPR, c='r', label='Training Set', linewidth=2)
        plt.plot(range(len(Test_TPR)), Test_TPR, c='g', linestyle='--', label='Validation Set', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('TPR')
        plt.legend()
        plt.savefig('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_'+str(self.apifeature_dims)+'/Epoch_TPR({0}, {1}).png'.format(self.blackbox, flag[self.same_train_data]))
        plt.show()

    def retrain_blackbox_detector(self):
        (xmal, ymal), (xben, yben) = self.load_data()
        xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
        xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
        # Generate Train Adversarial Examples
        noise = np.random.uniform(0, 1, (xtrain_mal.shape[0], self.z_dims))
        gen_examples = self.generator.predict([xtrain_mal, noise])
        gen_examples = np.ones(gen_examples.shape) * (gen_examples > 0.5)
        self.blackbox_detector.fit(np.concatenate([xtrain_mal, xtrain_ben, gen_examples]),
                                   np.concatenate([ytrain_mal, ytrain_ben, ytrain_mal]))

        # Compute Train TPR
        train_TPR = self.blackbox_detector.score(gen_examples, ytrain_mal)

        # Compute Test TPR
        noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.z_dims))
        gen_examples = self.generator.predict([xtest_mal, noise])
        gen_examples = np.ones(gen_examples.shape) * (gen_examples > 0.5)
        test_TPR = self.blackbox_detector.score(gen_examples, ytest_mal)
        print('\n---TPR after the black-box detector is retrained(Before Retraining MalGAN).')
        print('\nTrain_TPR: {0}, Test_TPR: {1}'.format(train_TPR, test_TPR))

if __name__ == '__main__':
    if args.train:
      feat_estm = FeatureEstimator()
      num_f = feat_estm.num_features_estimator(int(args.train[2]))
      # num_f = 13760
      malgan = MalGAN(apifeature_dims=num_f, blackbox='MLP')
      malgan.train(epochs=100, batch_size=64)
      # malgan.retrain_blackbox_detector()
      # malgan.train(epochs=100, batch_size=64, is_first=False)
    if args.attack:
      with open('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/len_feat_space.obj', 'rb') as lf_file:
        len_feat_space = pickle.load(lf_file)
      num_f = min(int(args.attack[1]), len_feat_space) if int(args.attack[1]) > 0 else len_feat_space
      generator_file = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(num_f) + '/generator_malgan.keras'
      if os.path.exists(generator_file):
        generator = load_model(generator_file)
        feature_space_path = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(num_f) + '/feature_space.obj'
        if os.path.exists(feature_space_path):
          with open(feature_space_path, 'rb') as feature_space_file:
            feature_space = pickle.load(feature_space_file)
        else:
          print('!!!LOADABLE FEATURE SPACE FILE NOT FOUND!!!')
          exit(0)
        
        if os.path.exists('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/dict_feat.obj'):
          with open('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/dict_feat.obj', 'rb') as df_file:
            dict_feat_space = pickle.load(df_file)
        else:
          print('!!!LOADABLE DICT FEATURE SPACE FILE NOT FOUND!!!')
          exit(0)
      else:
        print('!!!MODEL TO BE ATTACKED DOES NOT EXIST!!!')
        exit(0)
      z_dims = 20
      malware_file = args.attack[0]
      xmal = []
      feat_ext = FeatureExtractor()
      data = feat_ext.feature_extractor_file(malware_file)
      for _dict in data:
        feature_vector = dict.fromkeys(feature_space, 0)
        for key in _dict:
          value = _dict[key]
          features = value.split()
          for feature in features:
            if feature in feature_space:
              feature_vector[feature] = 1
        values = list(feature_vector.values())    
        xmal.append(values)

      xmal = np.asarray(xmal)

      noise = np.random.uniform(0, 1, (1, z_dims))
      gen_example = generator.predict([xmal, noise])[0].round()

      dict1 = copy.deepcopy(dict_feat_space)
      all_gen_features = []
      add_features = []
      for i, feature in enumerate(feature_space):
        if gen_example[i]==1:
          all_gen_features.append(feature)
        if xmal[0][i]==0 and gen_example[i]==1:
          add_features.append(feature)
      temp = copy.deepcopy(dict1["exports_list"])
      for _export in dict1["exports_list"]:
        if not _export in add_features:
           temp.remove(_export)
      dict1["exports_list"] = copy.deepcopy(temp)
      
      libs_to_remove = []
      for lib_key in dict1["libraries"]:
        if lib_key not in all_gen_features:
          libs_to_remove.append(lib_key)
          continue
        val = copy.deepcopy(dict1["libraries"][lib_key])
        for func in dict1["libraries"][lib_key]:
          if not func in add_features:
            val.remove(func)
        dict1["libraries"][lib_key] = copy.deepcopy(val)
      
    #   print(len(libs_to_remove), len(dict_feat_space['libraries']))
    #   print(dict_feat_space)

      for lib in libs_to_remove:
        dict1["libraries"].pop(lib)

      # keys = list(dict1['libraries'].keys())
      # for key in keys:
      #   if not dict1['libraries'][key]:
      #     dict1['libraries'].pop(key)
      # json_dict_feat_space = json.dumps(dict_feat_space)

      json_save_path = args.attack[4]
      os.makedirs(json_save_path, exist_ok = True)
      # with open(json_save_path+'json_feat_space.obj', 'wb') as j_file:
      #     pickle.dump(json_dict_feat_space, j_file)
      with open(json_save_path+'json_feat_space.json', 'w') as json_file:
        json.dump(dict1, json_file)
      # print(json_dict_feat_space)
    if args.test:
      with open('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/len_feat_space.obj', 'rb') as lf_file:
        len_feat_space = pickle.load(lf_file)
      num_f = min(int(args.test[1]), len_feat_space) if int(args.test[1]) > 0 else len_feat_space
      # combined_model_file = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(num_f) + '/combined_model_malgan.keras'
      blackbox_detector_file = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(num_f) + '/blackbox_detector.pkl'
      if os.path.exists(blackbox_detector_file):
        # combined_model = load_model(combined_model_file)
        with open(blackbox_detector_file, 'rb') as lf_file:
          blackbox_detector = pickle.load(lf_file)
        feature_space_path = './saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/num_features_' + str(num_f) + '/feature_space.obj'
        if os.path.exists(feature_space_path):
          with open(feature_space_path, 'rb') as feature_space_file:
            feature_space = pickle.load(feature_space_file)
        else:
          print('!!!LOADABLE FEATURE SPACE FILE NOT FOUND!!!')
          exit(0)
        
        if os.path.exists('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/dict_feat.obj'):
          with open('./saves_MalGAN_3/EXE/train_size_' + str(train_size) + '/fold_' + str(cur_fold) + '/dict_feat.obj', 'rb') as df_file:
            dict_feat_space = pickle.load(df_file)
        else:
          print('!!!LOADABLE DICT FEATURE SPACE FILE NOT FOUND!!!')
          exit(0)
      else:
        print('!!!MODEL TO BE TESTED DOES NOT EXIST!!!')
        exit(0)
      z_dims = 20
      input_file = args.test[0]
      x_test = []
      feat_ext = FeatureExtractor()
      data = feat_ext.feature_extractor_file(input_file)
      for _dict in data:
        feature_vector = dict.fromkeys(feature_space, 0)
        for key in _dict:
          value = _dict[key]
          features = value.split()
          for feature in features:
            if feature in feature_space:
              feature_vector[feature] = 1
        values = list(feature_vector.values())    
        x_test.append(values)

      x_test = np.asarray(x_test)

      noise = np.random.uniform(0, 1, (1, z_dims))
      
      # y_pred = (combined_model.predict([x_test, noise])[0]).round()
      y_pred = blackbox_detector.predict(x_test)

      pred_save_path = args.test[4]
      os.makedirs(pred_save_path, exist_ok = True)
      with open(pred_save_path+'y_pred.obj', 'wb') as pred_file:
          pickle.dump(y_pred, pred_file)
      print(y_pred)